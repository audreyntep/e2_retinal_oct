{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"IdFojbjNj6VJ"},"source":["# Detecting Retina Damage From Optical Coherence Tomography (OCT) Images, using Transfer Learning on VGG16 CNN Model"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T07:10:38.111278Z","iopub.status.busy":"2022-07-21T07:10:38.110436Z","iopub.status.idle":"2022-07-21T07:10:50.641451Z","shell.execute_reply":"2022-07-21T07:10:50.640336Z","shell.execute_reply.started":"2022-07-21T07:10:38.111172Z"},"executionInfo":{"elapsed":4845,"status":"ok","timestamp":1658101830778,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"_eLiARXPj6VP","outputId":"2ebc570c-3f3c-402f-9dd4-960a595f2ac4","trusted":true},"outputs":[],"source":["#!pip install keract"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-07-21T07:10:50.644358Z","iopub.status.busy":"2022-07-21T07:10:50.643765Z","iopub.status.idle":"2022-07-21T07:10:56.818264Z","shell.execute_reply":"2022-07-21T07:10:56.817285Z","shell.execute_reply.started":"2022-07-21T07:10:50.644314Z"},"id":"kwPBg7gNj6VR","trusted":true},"outputs":[],"source":["import os\n","from glob import glob\n","import pandas as pd\n","import numpy as np\n","from numpy import expand_dims\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sn\n","from skimage.transform import resize\n","from skimage.color import gray2rgb\n","from sklearn.metrics import classification_report, confusion_matrix\n","from IPython.display import SVG\n","import keract\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import applications, optimizers\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.utils import to_categorical, model_to_dot, plot_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2tdndSE9j6VS"},"source":["## Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T07:10:56.821456Z","iopub.status.busy":"2022-07-21T07:10:56.820669Z","iopub.status.idle":"2022-07-21T07:10:56.829110Z","shell.execute_reply":"2022-07-21T07:10:56.828238Z","shell.execute_reply.started":"2022-07-21T07:10:56.821403Z"},"id":"1E-3ZqlIj6VT","trusted":true},"outputs":[],"source":["data_dir = \"../data/OCT2017/\"\n","train_data_dir= '../data/OCT2017/train/'\n","val_data_dir= '../data/OCT2017/val/'\n","test_data_dir= '../data/OCT2017/test/'"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["img_width, img_height = 150, 150 \n","channels = 3\n","batch_size = 32"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Types of classes found:  4\n","cnv  37205 \n","dme  11348 \n","drusen  8616 \n","normal  26315\n","total : 83484\n"]}],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import os\n","\n","train_dataset_path = os.listdir(train_data_dir)\n","print(\"Types of classes found: \", len(train_dataset_path))\n","\n","cnv_images = len(glob(train_data_dir + 'CNV/*.jpeg'))\n","dme_images = len(glob(train_data_dir + 'DME/*.jpeg'))\n","drusen_images = len(glob(train_data_dir + 'DRUSEN/*.jpeg'))\n","normal_images = len(glob(train_data_dir + 'NORMAL/*.jpeg'))\n","\n","data= {'CNV': cnv_images, 'DME': dme_images, 'DRUSEN': drusen_images, 'NORMAL': normal_images}\n","\n","labels = list(data.keys()) \n","count = list(data.values())\n","\n","print('cnv ', cnv_images, '\\ndme ', dme_images,'\\ndrusen ', drusen_images,'\\nnormal ', normal_images)\n","print('total :', cnv_images+dme_images+drusen_images+normal_images)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["retinas = []\n","\n","for item in train_dataset_path:\n"," # Get all the file names\n"," all_retinas = os.listdir(train_data_dir  +item)\n"," #print(all_shoes)\n","\n"," # Add them to the list\n"," for retina in all_retinas:\n","    retinas.append((item, str(train_data_dir +item) + '/' + retina))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  classe                                           image\n","0    CNV    ../data/OCT2017/train/CNV/CNV-1016042-1.jpeg\n","1    CNV   ../data/OCT2017/train/CNV/CNV-1016042-10.jpeg\n","2    CNV  ../data/OCT2017/train/CNV/CNV-1016042-100.jpeg\n","3    CNV  ../data/OCT2017/train/CNV/CNV-1016042-101.jpeg\n","4    CNV  ../data/OCT2017/train/CNV/CNV-1016042-102.jpeg\n"]}],"source":["# Build a dataframe        \n","retinas_df = pd.DataFrame(data=retinas, columns=['classe', 'image'])\n","print(retinas_df.head())\n","#print(rooms_df.tail())"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of retinas in the train dataset:  83484\n","retinas in each category: \n","CNV       37205\n","NORMAL    26315\n","DME       11348\n","DRUSEN     8616\n","Name: classe, dtype: int64\n"]}],"source":["# Let's check how many samples for each category are present\n","print(\"Total number of retinas in the train dataset: \", len(retinas_df))\n","\n","retinas_count = retinas_df['classe'].value_counts()\n","\n","print(\"retinas in each category: \")\n","print(retinas_count)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["['CNV', 'DME', 'DRUSEN', 'NORMAL']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(train_data_dir)\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CNV\n","DME\n","DRUSEN\n","NORMAL\n"]}],"source":["import cv2\n","\n","im_size = 150\n","\n","images = []\n","labels = []\n","\n","for i in os.listdir(train_data_dir):\n","    print(i)\n","    data_path = train_data_dir + str(i)  \n","    filenames = [i for i in os.listdir(data_path)]\n","    \n","    for f in range(0,1000):\n","        img = cv2.imread(data_path + '/' + filenames[f])\n","        img = cv2.resize(img, (im_size, im_size))\n","        images.append(img)\n","        labels.append(i)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(32000, 150, 150, 3)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["images = np.array(images)\n","images = images.astype('float32') / 255.0\n","images.shape"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n","y = LabelEncoder().fit_transform(labels)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["(32000, 4)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["y=y.reshape(-1,1)\n","onehotencoder = OneHotEncoder(categories='auto')  #Converted  scalar output into vector output where the correct class will be 1 and other will be 0\n","Y= onehotencoder.fit_transform(y)\n","Y.shape  #(40, 2)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(30400, 150, 150, 3)\n","(30400, 4)\n","(1600, 150, 150, 3)\n","(1600, 4)\n"]}],"source":["from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","\n","images, Y = shuffle(images, Y, random_state=1)\n","\n","train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.05, random_state=415)\n","\n","#inpect the shape of the training and testing.\n","print(train_x.shape)\n","print(train_y.shape)\n","print(test_x.shape)\n","print(test_y.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UUOcv2u8j6VY"},"source":["## Model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Transfert learning"]},{"cell_type":"markdown","metadata":{},"source":["* VGG16 CNN architecture is used for classification.\n","* Pretrained on the 'ImageNet' dataset."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T07:12:43.850355Z","iopub.status.busy":"2022-07-21T07:12:43.847067Z","iopub.status.idle":"2022-07-21T07:12:47.353031Z","shell.execute_reply":"2022-07-21T07:12:47.351812Z","shell.execute_reply.started":"2022-07-21T07:12:43.850326Z"},"executionInfo":{"elapsed":5874,"status":"ok","timestamp":1658102990353,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"SKzrizgVj6VY","outputId":"90261bf2-615c-4ed7-b4e1-06382c16fdaa","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# instanciation d'un model VGG16 avec pré-entrainement imagenet\n","vgg16 = VGG16(include_top= False, input_shape= (img_width, img_height, channels), weights= 'imagenet')\n","vgg16.summary()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T11:53:08.479150Z","iopub.status.busy":"2022-07-21T11:53:08.478482Z","iopub.status.idle":"2022-07-21T11:53:08.578916Z","shell.execute_reply":"2022-07-21T11:53:08.577966Z","shell.execute_reply.started":"2022-07-21T11:53:08.479113Z"},"executionInfo":{"elapsed":546,"status":"ok","timestamp":1658102994202,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"lblkInHHj6VZ","outputId":"8521c8a6-6e9d-47ab-8338-75b16461349d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 8192)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 8192)              0         \n","                                                                 \n"," dense (Dense)               (None, 4)                 32772     \n","                                                                 \n","=================================================================\n","Total params: 14,747,460\n","Trainable params: 32,772\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}],"source":["# creation du model avec transfert learning de vgg16 et ajout de couches de sortie\n","model = Sequential()\n","\n","for layer in vgg16.layers:\n","    model.add(layer)\n","\n","for layer in model.layers:\n","    layer.trainable= False\n","\n","model.add(Flatten(input_shape= (4, 4, 512)))\n","model.add(Dropout(0.2))\n","model.add(Dense(4,activation='softmax'))\n","\n","model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iiHvHOrdj6VZ"},"source":["### Baseline Model Training"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T11:53:08.581666Z","iopub.status.busy":"2022-07-21T11:53:08.580916Z","iopub.status.idle":"2022-07-21T11:53:08.591602Z","shell.execute_reply":"2022-07-21T11:53:08.590701Z","shell.execute_reply.started":"2022-07-21T11:53:08.581612Z"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1658102999806,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"oA7oUUXEj6VZ","outputId":"44ae0acb-5374-4eff-f7fd-3d7c90dba4cc","trusted":true},"outputs":[],"source":["model.compile(\n","    optimizer= keras.optimizers.Adam(learning_rate= 0.0001), \n","    loss='categorical_crossentropy', \n","    metrics= ['accuracy']\n","    )"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T11:53:08.594058Z","iopub.status.busy":"2022-07-21T11:53:08.593690Z","iopub.status.idle":"2022-07-21T11:53:08.602611Z","shell.execute_reply":"2022-07-21T11:53:08.601544Z","shell.execute_reply.started":"2022-07-21T11:53:08.594022Z"},"executionInfo":{"elapsed":3729232,"status":"ok","timestamp":1658106967707,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"hasHQMV2j6VZ","outputId":"50da1168-21b3-47a7-cf76-ab73df598cbc","trusted":true},"outputs":[],"source":["# définition des hyperparamètres du model\n","model_name='vgg16_e10b128'\n","checkpoint_filepath = model_name+'/tmp/checkpoint'\n","checkpoint = ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)\n","earlystop = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","callbacks_list = [earlystop,checkpoint]"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T11:53:08.687779Z","iopub.status.busy":"2022-07-21T11:53:08.687425Z","iopub.status.idle":"2022-07-21T14:43:37.892798Z","shell.execute_reply":"2022-07-21T14:43:37.891697Z","shell.execute_reply.started":"2022-07-21T11:53:08.687750Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"ename":"TypeError","evalue":"in user code:\n\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\backend.py\", line 5530, in categorical_crossentropy\n        target = tf.convert_to_tensor(target)\n\n    TypeError: Failed to convert elements of SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m numepochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m----> 4\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_x , train_y,\n\u001b[0;32m      5\u001b[0m                     epochs\u001b[39m=\u001b[39;49mnumepochs, \n\u001b[0;32m      6\u001b[0m                     batch_size \u001b[39m=\u001b[39;49m batch_size\n\u001b[0;32m      7\u001b[0m                     )\n","File \u001b[1;32md:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filet7weglyh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\02.Pro\\SIMPLON\\Certification\\E2\\e2_retinal_oct\\.venv\\lib\\site-packages\\keras\\backend.py\", line 5530, in categorical_crossentropy\n        target = tf.convert_to_tensor(target)\n\n    TypeError: Failed to convert elements of SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"]}],"source":["# entrainement \n","numepochs = 5\n","batch_size = 128\n","history = model.fit(train_x , train_y,\n","                    epochs=numepochs, \n","                    batch_size = batch_size\n","                    )\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8jHdPPPtxJc1"},"source":["### Evaluations on Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:37.894931Z","iopub.status.busy":"2022-07-21T14:43:37.894598Z","iopub.status.idle":"2022-07-21T14:43:42.084266Z","shell.execute_reply":"2022-07-21T14:43:42.083191Z","shell.execute_reply.started":"2022-07-21T14:43:37.894902Z"},"executionInfo":{"elapsed":5310,"status":"ok","timestamp":1658106995551,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"c4EBzCSWj6Va","outputId":"fbbab39b-4fb6-4824-be23-fa1f40ed8644","scrolled":true,"trusted":true},"outputs":[],"source":["(eval_loss, eval_accuracy) = model.evaluate(test_x, test_y, batch_size= batch_size, verbose= 1)\n","print('Test Loss: ', eval_loss)\n","print('Test Accuracy: ', eval_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:42.086756Z","iopub.status.busy":"2022-07-21T14:43:42.086038Z","iopub.status.idle":"2022-07-21T14:43:43.425835Z","shell.execute_reply":"2022-07-21T14:43:43.424915Z","shell.execute_reply.started":"2022-07-21T14:43:42.086718Z"},"executionInfo":{"elapsed":2829,"status":"ok","timestamp":1658107018494,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"PLrNe_BLw209","outputId":"2a37d92d-fbd1-4685-f7ff-3be330f4f366","trusted":true},"outputs":[],"source":["plt.subplot()\n","plt.rcParams['figure.figsize'] = (6.0, 4.0)\n","plt.title('Baseline Model Accuracy')\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend(['Training Accuracy','Validation Accuracy'])\n","plt.savefig(model_name+'/img/baseline_acc_epoch_'+model_name+'.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.subplot()\n","plt.title('Baseline Model Loss')\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend(['Training Loss','Validation Loss'])\n","plt.savefig(model_name+'/img/baseline_loss_epoch_'+model_name+'.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:43.429905Z","iopub.status.busy":"2022-07-21T14:43:43.428428Z","iopub.status.idle":"2022-07-21T14:43:48.175745Z","shell.execute_reply":"2022-07-21T14:43:48.174647Z","shell.execute_reply.started":"2022-07-21T14:43:43.429864Z"},"executionInfo":{"elapsed":6308,"status":"ok","timestamp":1658107042296,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"XEyYvvPCxFwa","outputId":"1784c902-99b3-4738-b2bf-e333c8a872d4","trusted":true},"outputs":[],"source":["Y_pred = model.predict(test_generator, nb_test_samples // batch_size+1)\n","y_pred = np.argmax(Y_pred, axis=1)\n","cm = confusion_matrix(test_generator.classes, y_pred)\n","df_cm = pd.DataFrame(cm, list(test_generator.class_indices.keys()), list(test_generator.class_indices.keys()))\n","fig, ax = plt.subplots(figsize=(10,8))\n","sn.set(font_scale=1.4) # for label size\n","sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=plt.cm.Blues)\n","plt.title('Confusion Matrix\\n')\n","plt.savefig(model_name+'/img/confusion_matrix_'+model_name+'.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Classification Report\\n')\n","target_names = list(test_generator.class_indices.keys())\n","print(classification_report(test_generator.classes, y_pred, target_names=target_names))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"e6UmPPNNj6Va"},"source":["### Save the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_save_h5 = model_name+\"/retinal_oct_model_\"+model_name+\".h5\"\n","model_save_json = model_name+\"/retinal_oct_model_\"+model_name+\".json\"\n","model_save_weights = model_name+\"/retinal_oct_model_\"+model_name+\"_weights.h5\"\n","model_save_metrics = model_name+\"/retinal_oct_model_\"+model_name+\"_eval.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:48.177745Z","iopub.status.busy":"2022-07-21T14:43:48.177172Z","iopub.status.idle":"2022-07-21T14:43:48.295531Z","shell.execute_reply":"2022-07-21T14:43:48.294483Z","shell.execute_reply.started":"2022-07-21T14:43:48.177708Z"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1658107050736,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"PmcQoAD8j6Va","outputId":"cdbc2277-e588-42df-b425-7787d80b73b1","trusted":true},"outputs":[],"source":["# save model and architecture to h5 file\n","model.save(model_save_h5)\n","print(\"Saved h5 model to disk\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:48.297938Z","iopub.status.busy":"2022-07-21T14:43:48.296856Z","iopub.status.idle":"2022-07-21T14:43:48.409260Z","shell.execute_reply":"2022-07-21T14:43:48.408295Z","shell.execute_reply.started":"2022-07-21T14:43:48.297896Z"},"trusted":true},"outputs":[],"source":["#save the model architecture to JSON file\n","from keras.models import model_from_json\n","# serialize model to json\n","json_model = model.to_json()\n","with open(model_save_json, 'w') as json_file:\n","    json_file.write(json_model)\n","print(\"Saved json model to disk\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#saving the weights of the model\n","model.save_weights(model_save_weights)\n","print(\"Saved model weights to disk\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:48.411169Z","iopub.status.busy":"2022-07-21T14:43:48.410800Z","iopub.status.idle":"2022-07-21T14:43:48.419242Z","shell.execute_reply":"2022-07-21T14:43:48.418020Z","shell.execute_reply.started":"2022-07-21T14:43:48.411129Z"},"trusted":true},"outputs":[],"source":["import json\n","# save model metrics\n","json_model_eval = {}\n","json_model_eval[\"model_name\"]=model_name\n","json_model_eval[\"loss\"] = eval_loss\n","json_model_eval[\"accuracy\"] = eval_accuracy\n","with open(model_save_metrics, 'w') as json_file:\n","    json_file.write(str(json_model_eval))\n","print(\"Saved model metrics to disk\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load and evaluate models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-21T14:43:48.421840Z","iopub.status.busy":"2022-07-21T14:43:48.421261Z","iopub.status.idle":"2022-07-21T14:43:48.453138Z","shell.execute_reply":"2022-07-21T14:43:48.451626Z","shell.execute_reply.started":"2022-07-21T14:43:48.421796Z"},"trusted":true},"outputs":[],"source":["# It can be used to reconstruct the model identically.\n","reconstructed_model = keras.models.load_model(model_save_h5)\n","\n","# Let's check:\n","# np.testing.assert_allclose(\n","#     model.predict(test_generator), \n","#     reconstructed_model.predict(test_generator)\n","# )\n","\n","# The reconstructed model is already compiled and has retained the optimizer\n","# state, so training can resume:\n","#reconstructed_model.fit(test_generator)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Evaluation du modèle vgg16 epochs 5 batchsize 128 nouvel entrainement :"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(eval_loss, eval_accuracy) = reconstructed_model.evaluate(test_generator, batch_size= batch_size, verbose= 1)\n","print('Test Loss: ', eval_loss)\n","print('Test Accuracy: ', eval_accuracy)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Evaluation du modèle vgg16 epochs 5 batchsize 128 model existant au début du projet :"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# récupération des métriques d'évaluation du modèle\n","e5b128_model_old = keras.models.load_model(\"../.old/model/retinal-oct.h5\")\n","e5b128_model_old.evaluate(test_generator)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Evaluation du modèle vgg16 epochs 5 batchsize 128, modèle entrainé lors de la phase d'étude du projet :"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# récupération des métriques d'évaluation du modèle\n","e5b128_model = keras.models.load_model(\"vE5-B128/retinal_oct_model_vE5-B128.h5\")\n","e5b128_model.evaluate(test_generator)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["L'écart entre les valeurs des métriques de ces 3 modèles avec les même hyperparamètres peut s'expliquer avec un jeu de données différent."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oaz-a9a_GCB8"},"source":["### Prediction test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:43:48.454456Z","iopub.status.idle":"2022-07-21T14:43:48.455445Z","shell.execute_reply":"2022-07-21T14:43:48.455213Z","shell.execute_reply.started":"2022-07-21T14:43:48.455189Z"},"executionInfo":{"elapsed":666,"status":"ok","timestamp":1658110314261,"user":{"displayName":"Noura BENHAJJI","userId":"01171713927746675194"},"user_tz":-120},"id":"EYKm6c3MAIX7","outputId":"16d87e5b-6233-4298-9a56-125a007ebffe","trusted":true},"outputs":[],"source":["from PIL import Image\n","import io\n","# model = tf.keras.models.load_model(model_save_h5)\n","reconstructed_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","test_image = keras.utils.load_img(test_data_dir+\"/CNV/CNV-1016042-1.jpeg\", target_size = (150, 150)) \n","test_image = keras.utils.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","\n","#predict the result\n","result = np.argmax(model.predict(test_image))\n","print(result)\n","print(list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(result)])"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"654279205dda07afd21cd6d9ab173014e3297b30820a73c49be6651e0ab23bbf"}}},"nbformat":4,"nbformat_minor":4}
